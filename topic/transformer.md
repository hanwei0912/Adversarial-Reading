### ChenKalantidisLiYF_2018 - [$A^2$-Nets: Double Attention Networks](https://arxiv.org/pdf/1810.11579.pdf)

- basic idea: learn the global features and local feature, then weight local features according to the similarity to global features, so that local features share information with global features.
- conv backbone + transformer


### ShenZhangZhaoYL_2018 - [Efficient Attention- Attention with Linear Complexities](https://arxiv.org/pdf/1812.01243.pdf)

- basic idea: show when change the order of matrix muliplication, the results is equivalent but the complexity reduce. Provide a new explanation of attention, which is exactly the same to A^2-Net 

YueSunYuanZDX_2018 - Compact Generalized Non-local Network

ChoromanskiLikhosherstovDohanSGSHDBCW_2020 - Rethinking Attention with Performers

El-NoubyTouvronCaronBDJLNSVJ_2021 - XCiT- Cross-Covariance Image Transformers

KatharopoulosVyasPappasF_2020 - Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention

REVIEW - Anonymous_2022 - Patches Are All You Need?

REVIEW - ZhaiTalbottSrivastavaHGS_2021 - An Attention Free Transformer

TolstikhinHoulsbyKolesnikovBZUYSKULD_2021 - MLP-Mixer- An all-MLP Architecture for Vision

WangLiKhabsaFM_2020 - Linformer- Self-Attention with Linear Complexity

ZhaiTalbottSrivastavaHGZS_2021 - An Attention Free Transformer

(linear attention)

HeoYunHanCCO_tech2021 - Rethinking Spatial Dimensions of Vision Transformers

WuXiaoCodellaLDYZ_tech2021 - CvT- Introducing Convolutions to Vision Transformers

(spatial pooling)

ResNet strikes back: An improved training procedure in timm 

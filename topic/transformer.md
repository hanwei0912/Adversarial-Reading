ChenKalantidisLiYF_2018 - $A^2$-Nets: Double Attention Networks

ShenZhangZhaoYL_2018 - Efficient Attention- Attention with Linear Complexities

YueSunYuanZDX_2018 - Compact Generalized Non-local Network

ChoromanskiLikhosherstovDohanSGSHDBCW_2020 - Rethinking Attention with Performers

El-NoubyTouvronCaronBDJLNSVJ_2021 - XCiT- Cross-Covariance Image Transformers

KatharopoulosVyasPappasF_2020 - Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention

REVIEW - Anonymous_2022 - Patches Are All You Need?

REVIEW - ZhaiTalbottSrivastavaHGS_2021 - An Attention Free Transformer

TolstikhinHoulsbyKolesnikovBZUYSKULD_2021 - MLP-Mixer- An all-MLP Architecture for Vision

WangLiKhabsaFM_2020 - Linformer- Self-Attention with Linear Complexity

ZhaiTalbottSrivastavaHGZS_2021 - An Attention Free Transformer

(linear attention)

HeoYunHanCCO_tech2021 - Rethinking Spatial Dimensions of Vision Transformers

WuXiaoCodellaLDYZ_tech2021 - CvT- Introducing Convolutions to Vision Transformers

(spatial pooling)

ResNet strikes back: An improved training procedure in timm 

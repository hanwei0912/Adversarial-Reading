## Don't Forget

### [GAN Memory with No Forgetting](https://proceedings.neurips.cc/paper/2020/file/bf201d5407a6509fa536afc4b380577e-Paper.pdf)

### [Recall and Learn: Fine-tuning Deep Pretrained Language Models with Less Forgetting](https://arxiv.org/pdf/2004.12651)

### [Growing a Brain: Fine-Tuning by Increasing Model Capacity](https://openaccess.thecvf.com/content_cvpr_2017/papers/Wang_Growing_a_Brain_CVPR_2017_paper.pdf)

### [REMIND Your Neural Network to Prevent Catastrophic Forgetting](https://arxiv.org/pdf/1910.02509)

## Learn to forget
### [Learning to Forget for Meta-Learning](https://openaccess.thecvf.com/content_CVPR_2020/papers/Baik_Learning_to_Forget_for_Meta-Learning_CVPR_2020_paper.pdf)

### [Can Bad Teaching Induce Forgetting? Unlearning in Deep Networks Using an Incompetent Teacher]()



## Fine-Tuning for Language model

### [Parameter-Efficient Fine-Tuning for Large Models: A Comprehensive Survey](https://arxiv.org/pdf/2403.14608)

### [Is ChatGPT a Good NLG Evaluator? A Preliminary Study](https://arxiv.org/pdf/2303.04048)

### [Can ChatGPT Understand Too? A Comparative Study on ChatGPT and Fine-tuned BERT](https://arxiv.org/pdf/2302.10198)

### [Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond](https://dl.acm.org/doi/pdf/10.1145/3649506)

### [Studying the impacts of pre-training using ChatGPT generated text on downstream tasks](https://arxiv.org/pdf/2309.05668)


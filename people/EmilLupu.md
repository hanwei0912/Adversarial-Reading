### [Evolution of attacks, threat models, and solutions for virtualized systems](https://cyberleninka.org/article/n/1334959.pdf)
- 2016 ACM CSUR
- Virtualization technology enables Cloud providers to efficiently use their computing services and resources. Even if the benefits in terms of performance, maintenance, and cost are evident, however, virtualization has also been exploited by attackers to devise new ways to compromise a system. To address these problems, research security solutions have evolved considerably over the years to cope with new attacks and threat models. In this work, we review the protection strategies proposed in the literature and show how some of the solutions have been invalidated by new attacks, or threat models, that were previously not considered. The goal is to show the evolution of the threats, and of the related security and trust assumptions, in virtualized systems that have given rise to complex threat models and the corresponding sophistication of protection strategies to deal with such attacks.We also categorize threat models, security and trust assumptions, and
attacks against a virtualized system at the different layersâ€”in particular, hardware, virtualization, OS, and application.

### [Poisoning attacks with generative adversarial nets](https://arxiv.org/pdf/1906.07773)
- 2019
- Machine learning algorithms are vulnerable to poisoning attacks: An adversary can inject malicious points in the training dataset to influence the learning process and degrade the algorithm's performance. Optimal poisoning attacks have already been proposed to evaluate worst-case scenarios, modelling attacks as a bi-level optimization problem. Solving these problems is computationally demanding and has limited applicability for some models such as deep networks. In this paper we introduce a novel generative model to craft systematic poisoning attacks against machine learning classifiers generating adversarial training examples, i.e. samples that look like genuine data points but that degrade the classifier's accuracy when used for training. We propose a Generative Adversarial Net with three components: generator, discriminator, and the target classifier. This approach allows us to model naturally the detectability constrains that can be expected in realistic attacks and to identify the regions of the underlying data distribution that can be more vulnerable to data poisoning. Our experimental evaluation shows the effectiveness of our attack to compromise machine learning classifiers, including deep networks.

### [Using 3D Shadows to Detect Object Hiding Attacks on Autonomous Vehicle Perception](https://scholar.google.com/citations?view_op=view_citation&hl=zh-CN&user=e23mVyoAAAAJ&sortby=pubdate&citation_for_view=e23mVyoAAAAJ:geVfx-PNG5EC)
- 2022

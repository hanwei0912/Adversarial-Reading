### [Evolution of attacks, threat models, and solutions for virtualized systems](https://cyberleninka.org/article/n/1334959.pdf)
- 2016 ACM CSUR
- Virtualization technology enables Cloud providers to efficiently use their computing services and resources. Even if the benefits in terms of performance, maintenance, and cost are evident, however, virtualization has also been exploited by attackers to devise new ways to compromise a system. To address these problems, research security solutions have evolved considerably over the years to cope with new attacks and threat models. In this work, we review the protection strategies proposed in the literature and show how some of the solutions have been invalidated by new attacks, or threat models, that were previously not considered. The goal is to show the evolution of the threats, and of the related security and trust assumptions, in virtualized systems that have given rise to complex threat models and the corresponding sophistication of protection strategies to deal with such attacks.We also categorize threat models, security and trust assumptions, and
attacks against a virtualized system at the different layers—in particular, hardware, virtualization, OS, and application.

### [Poisoning attacks with generative adversarial nets](https://arxiv.org/pdf/1906.07773)
- 2019
- Machine learning algorithms are vulnerable to poisoning attacks: An adversary can inject malicious points in the training dataset to influence the learning process and degrade the algorithm's performance. Optimal poisoning attacks have already been proposed to evaluate worst-case scenarios, modelling attacks as a bi-level optimization problem. Solving these problems is computationally demanding and has limited applicability for some models such as deep networks. In this paper we introduce a novel generative model to craft systematic poisoning attacks against machine learning classifiers generating adversarial training examples, i.e. samples that look like genuine data points but that degrade the classifier's accuracy when used for training. We propose a Generative Adversarial Net with three components: generator, discriminator, and the target classifier. This approach allows us to model naturally the detectability constrains that can be expected in realistic attacks and to identify the regions of the underlying data distribution that can be more vulnerable to data poisoning. Our experimental evaluation shows the effectiveness of our attack to compromise machine learning classifiers, including deep networks.

### [Using 3D Shadows to Detect Object Hiding Attacks on Autonomous Vehicle Perception](https://scholar.google.com/citations?view_op=view_citation&hl=zh-CN&user=e23mVyoAAAAJ&sortby=pubdate&citation_for_view=e23mVyoAAAAJ:geVfx-PNG5EC)
- 2022
- Autonomous Vehicles (AVs) are mostly reliant on LiDAR sensors which enable spatial perception of their surroundings and help make driving decisions. Recent works demonstrated attacks that aim to hide objects from AV perception, which can result in severe consequences. 3D shadows, are regions void of measurements in 3D point clouds which arise from occlusions of objects in a scene. 3D shadows were proposed as a physical invariant valuable for detecting spoofed or fake objects. In this work, we leverage 3D shadows to locate obstacles that are hidden from object detectors. We achieve this by searching for void regions and locating the obstacles that cause these shadows. Our proposed methodology can be used to detect an object that has been hidden by an adversary as these objects, while hidden from 3D object detectors, still induce shadow artifacts in 3D point clouds, which we use for obstacle detection. We show that using 3D shadows for obstacle detection can achieve high accuracy in matching shadows to their object and provide precise prediction of an obstacle's distance from the ego-vehicle.

### [Jacobian Ensembles Improve Robustness Trade-offs to Adversarial Attacks](https://arxiv.org/pdf/2204.08726.pdf)
- 2022
- UAPs are a class of perturbations that when applied to any input causes model misclassification.
Although there is an ongoing effort to defend models against these adversarial attacks, it is often difficult to reconcile the trade-offs in model accuracy and robustness to adversarial attacks. Jacobian regularization has
been shown to improve the robustness of models against UAPs, whilst
model ensembles have been widely adopted to improve both predictive
performance and model robustness. In this work, we propose a novel
approach, Jacobian Ensembles – a combination of Jacobian regularization and model ensembles to significantly increase the robustness against
UAPs whilst maintaining or improving model accuracy. Our results show
that Jacobian Ensembles achieves previously unseen levels of accuracy
and robustness, greatly improving over previous methods that tend to
skew towards only either accuracy or robustness.

### [Redundancy Planning for Cost Efficient Resilience to Cyber Attacks](https://spiral.imperial.ac.uk/bitstream/10044/1/94453/2/Redundancy_planning_ACCEPTED_MAIN.pdf)
- 2022 IEEE TDSC
- We investigate the extent to which redundancy (including with diversity) can help mitigate the impact of cyber attacks that aim to reduce system performance. Using analytical techniques, we estimate impacts, in terms of monetary costs, of penalties from breaching Service Level Agreements (SLAs), and find optimal resource allocations to minimize the overall costs arising from attacks. Our approach combines attack impact analysis, based on performance modeling using queueing networks, with an attack model based on attack graphs. We evaluate our approach using a case study of a website, and show how resource redundancy and diversity can improve the resilience of a system by reducing the likelihood of a fully disruptive attack. 

### [Shadow-catcher: Looking into shadows to detect ghost objects in autonomous vehicle 3d sensing](https://arxiv.org/pdf/2008.12008.pdf)
- 2021 ESRCS
- LiDAR-driven 3D sensing allows new generations of vehicles to achieve advanced levels of situation awareness. However, recent works have demonstrated that physical adversaries can spoof LiDAR return signals and deceive 3D object detectors to erroneously detect “ghost" objects. Existing defenses are either impractical or focus only on vehicles. Unfortunately, it is easier to spoof smaller objects such as pedestrians and cyclists, but harder to defend against and can have worse safety implications. To address this gap, we introduce Shadow-Catcher, a set of new techniques embodied in an end-to-end prototype to detect both large and small ghost object attacks on 3D detectors. We characterize a new semantically meaningful physical invariant (3D shadows) which Shadow-Catcher leverages for validating objects.

### [Universal adversarial robustness of texture and shape-biased models](https://arxiv.org/pdf/1911.10364.pdf)

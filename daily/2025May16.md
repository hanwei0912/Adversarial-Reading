### [Towards Tracing Trustworthiness Dynamics: Revisiting Pre-training Period of Large Language Models](https://arxiv.org/pdf/2402.19465)

To our best knowledge, two aspects still remain mysterious: 1) how LLMs dynamically encode trustworthiness during pre-training, and 2) how to harness the pre-training period for more trustworthy LLMs.
- linear probing: Our probing results suggest that after the early pre-training period, middle layer representations of LLMs have already developed linearly separable patterns about trustworthiness
- activation intervention
- steering vector
- mutual information
- reliability(truthful QA, truthfulness discernment),includes 817
questions across 38 categories aimed at assessing
the veracity of model-generated answers.
- toxicity (ToxiGen), featuring implicit toxic and non-toxic
statements across 13 minority demographics, enabling toxicity modeling assessment in LLMs.
- privacy,  ConfAIde targeting contextual privacy and identifying vulnerabilities in
LLMsâ€™ privacy reasoning
- fairness, StereoSet  stereotype modeling ability
-  robustness

### [On Evaluating Adversarial Robustness of Large Vision-Language Models](https://proceedings.neurips.cc/paper_files/paper/2023/file/a97b58c4f7551053b0512f92244b0810-Paper-Conference.pdf)
- we propose evaluating the robustness of open-source large VLMs in the most realistic and high-risk setting, where adversaries have only black-box system access and seek to deceive the model into returning the targeted responses.
- forcing the image embedding vector close to the target caption embedding vector

### [DECODINGTRUST: A Comprehensive Assessment of Trustworthiness in GPT Models](https://blogs.qub.ac.uk/wp-content/uploads/sites/7/2024/01/A-comprehensive-Assessment-of-Trustworthiness-in-GPT-Models.pdf)

### [Survey of Vulnerabilities in Large Language Models Revealed by Adversarial Attacks](https://arxiv.org/pdf/2310.10844)
- textual only attacks
  1. Jailbreak: (1) manually crafted jailbreak prompts --> formed a benchmark --> evaluate the effectiveness of existing prompts, explore the underlying factors, insights --> systematic and automated ways of generating more advanced jailbreaks --> MJP (Multi-step Jailbreaking Prompt): first plays the role of the user and use existing jailbreak, then concatenate an acknowledge into prompt as if the hypothetical is accepted; context contamination; prf;i x injection
- multi-modal attacks
- additional attacks

###[SHIELD: A REGULARIZATION TECHNIQUE FOR EXPLAINABLE ARTIFICIAL INTELLIGENCE](https://arxiv.org/pdf/2404.02611)

SHIELD (Selective Hidden Input Evaluation for Learning Dynamics), a
regularization technique for explainable artificial intelligence designed to improve model quality by
concealing portions of input data and assessing the resulting discrepancy in predictions


• Does the explanation behave like the model in the example to be explained? Local Fidelity and Local
Concordance measure this behavior.
• Can this explanation be extrapolated far from the example to be explained? Prescriptivity measures if a
synthetic example constructed to differ from the original example ends up modifying the behavior of the
model.
• How much can an explanation vary if it is generated several times? Robustness measures how much two
explanations of the same example differ if the method of generating explanations is stochastic.
• Other descriptive information on the explanations. Conciseness measures the number of features with high
involvement in the decisions of the model, being a very concise model when few features have high involvement
and will be not very concise when there are many features with distributed influence in the decision of the
model.

### [Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI](https://pdf.sciencedirectassets.com/272144/1-s2.0-S1566253519X0007X/1-s2.0-S1566253519308103/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjED0aCXVzLWVhc3QtMSJGMEQCIB%2F70C97bZVLrL7OjKjZZsZSbCvvHFN4rQqVIUF71E9eAiBw1no2hSJ9%2FZ68Tx9pTKZsabGM6Sg4gZWUmwg9eIfd0Sq8BQim%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F8BEAUaDDA1OTAwMzU0Njg2NSIMqpSTu8NqaeHsHmI%2FKpAFhu4eXef%2BxqIb%2FRBQrTr8fUrPejNVAmGENZeihAaILTJXtkhkGBd740x9orNRQCPd%2FxhOxI0uUfln4RMcP0VbY0xLIHCr5q%2Brm2Wg5tqJIJkJUxdaJr4tE5sAIKkdMTD%2Fu2jwcv11EjuPEeXXlmjsU7zKMZu9rdtPzRGx80EV4d0Kdc3vTcqT42Yb2vO%2F7SU6XQnCHzWvxkk74NkIInca%2FCvjIbJKOhlatssFlQlX2KLPIfCMKiyBybo1coLUfGzqKD5DrTq5EiSKKtBoGm%2FvRt4e3MZUQkDpyYNfbBOJgtkKb8q08mgsksDo6lWopcZZH9eNjiDXEzrtMO95N7fHEGI83mikEOLRCCmaEBr6IWYSIeJZKjYRTKV2GfsL7m720ftMs0LkeG6FVijID40Mtz85S94ropAH9bEjBGJCDmMntU1W%2BAMSJ7nSWqgsxdPfe3a3js1RWtLKAzBiZBpqLig0Oou%2B3NveTPojTbkKdDwFxSHH62z4sJkY4FtmGQxvmuf7af%2FdS37mm99dLZ5VZUkvdMw39LkNFnajFKlWBHBqYpDf3L39atINu6Zw2wIT1a3o3ju8891i%2FfGghrjccC8xdaNM7XkAodmD5MgWD3brI00ZgFABNsX2e5gu6KN0GaGOUdviGYYggBg056u8dhvNSvy23U4FCV6epUJ6j7a1tzu59afzPm%2BwIngpcS8ZhnN1AjaU6uvDK2bHRZ8bjg%2BWnZ%2Bb3IAxsee84mQtJHPh7MIKz%2FN39gz2sEEpWrgM8ebZnjo0lOXJQENFXFhPGDb12Oi4%2FghQv0IR8Xn773JIkc3enWpSZgU84flANpSdUjjZVGi1ttZRpkRrDGxguHn7Ll9MiV3EhPMip6bmQNAwxt%2BSsgY6sgFkU5cANbxvoqWJcVV%2BwOUa%2FtCqKrTP9tgU8%2FZtNQ4pmD94TLUCVEUNRlQrhsqWWc%2FICHHYksNPbZ5KYS1oZ0fp3aOwU7jxpVVkAXrPXNNbTC%2B8y9qZ8tT5QmZyLCZmS0kunk6FlFAiw%2FjNx3CgSiicmU3mxdw7kpPXmxLQl5jvQGuuanxc0QUDLFYvY%2Fl7L7Zb9ijdShMN9iBorcSB0eyrwVAWbRsB%2F9xEW2I9qR4Ott2I&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20240515T143829Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTY6TIB7UJV%2F20240515%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=65c6352c543bf11cc8b571dd305226d9d888309d837c2df0c066307270479ba1&hash=5c18f22dd36bf1cd54d162802dd95b4bc7ff037e880760cdadcafd7d8c8db9db&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S1566253519308103&tid=spdf-998b3cdb-83ba-4bf3-8ea5-8e5705b3d4c3&sid=b7e739c520beb947e05815f003f2891ace4fgxrqb&type=client&tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&ua=02015b5f515307015703&rr=8843df3b2fec9250&cc=de)

### [State representation learning for control: An overview](https://pdf.sciencedirectassets.com/271125/1-s2.0-S0893608018X00094/1-s2.0-S0893608018302053/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjED8aCXVzLWVhc3QtMSJHMEUCIQDpQhXrDOn%2FhJt%2B9D1p%2B14OkAfxhJgjwmxMdViAgsjijQIgShtAeNzUvManN94sMboc2kAze4qncRR3eip1toLhCtgquwUIqP%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARAFGgwwNTkwMDM1NDY4NjUiDFH%2Breuk%2BcERr7RCtSqPBXVg3H2H7SmmP97UdkDZY%2BcLbb4NFucTnMDNDyNGNYmQ0IwAEUH9kgRlQFVr932smtenaRmQHp0%2FRLsH6UlXTnppJe%2BlSDb1ojtZzUrrCmReXVDgF%2BpNxalLKEC%2BZbyPiggvHsWlo%2Fio7nhaMsuxUlRbGSs03fj79tT0629luyHCrUNKaTrrtPC%2BIoLa7ZzGiwDDJodvlPqbbsYi3NmUzY5iht5YeAuXxms4SGoGOgv7NdZEBu9H0TB7k5ez7SzAWsTNQrShae%2FuLIgSa%2F0fCKAF%2BjKxJlvYT1kJ0eDjitaH9JCnpAbVIax8CQMEjs0n1Ts%2BY1hSdkFbMAcBPDpsXo5eD368hz9aSnaXdvAiZYzYfeSkxoLRm0uKsWN24Rv9Ts6oeBpFQ2fd9El2TVdrEE5pzoZhfv06AHiyoNfjquC2NvNowL2FeXBIl0sTA%2BXZictG68M9TOJHAKSaNDS%2BnhIxO3zYZwPG%2FJWvjICqExYQpf6a5PmJKLhEYiLyNLAk%2FBMXqFiQbsZz4Bjd%2FBTAQ5eP%2BpmgSjIg3tv0pduBZUW5Qkt3KpqrXMV%2Blp8AQQzbERkJQpzsDcXJ745nT8Q8dEGpye4Os%2FUtlBxsYnfMvIFqHvuP6%2FF4OzAdmdc4gW7p4p5IQL2fFu1Ui4MAv3gmaS6LSR1wQPXFY5o9h2hF%2F9qB5XXKZZmu6DPckEwMXls%2FQCcrZ2Zg7QxteIXxHiVM4o%2B8OrvDRCGO6ATe7VIgZP5QgvRAa2qTrPOcgG6GU0ra0ncssFZMnvIAU3PkY%2Bo5WOsolg8gjloLlVDVy%2BtpNgkknuTmPw78CXD7IuyYcNW85Auz9TDXb%2F6n4GqfymCOieTMC2y9pPLCmhGGqxNq7DswjoyTsgY6sQEjSwT8R6wMXHHrjxeEdsx6Z4BGUO4RUyZ1TTUNwBMBTWlyWenp1kh9qaiH9OUxrvkgFMG50Ox5Qd3qAm3DV7RBC5FkCt5dIB%2FFjGoorHnUOzvWlp6DQC8iYssZMTwVc09go%2Bm%2BR3QXc6my0ZjlorbdVT6Mo3II%2F5xVqt1EQj8Li5g0dkk4AxEB03u8yQmmdJUPB8eEXgxWZozqCeSv7fUkADpKhEXPDIbMBm7G9kM%2BZG0%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20240515T143833Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTY2CHYNH2Q%2F20240515%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=44bdc19326969a864cae2ac08a2293fe65008560747043ee969d15bb0bd17eb1&hash=4d39066005f67893d44d75383dcd6a65280789ce9f59ee0a5efec1abf4b6260b&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S0893608018302053&tid=spdf-da65ac45-6f84-49c5-8c92-ccf624a789a8&sid=b7e739c520beb947e05815f003f2891ace4fgxrqb&type=client&tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&ua=02015b5f515307015155&rr=8843df54cc379250&cc=de)

### [On generating trustworthy counterfactual explanations]()

### [Connecting the dots in trustworthy Artificial Intelligence: From AI principles, ethics, and key requirements to responsible AI systems and regulation]()

### [Explainable Artificial Intelligence (XAI): What we know and what is left to attain Trustworthy Artificial Intelligence]()

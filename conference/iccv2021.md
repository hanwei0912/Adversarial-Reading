## Robustness

### [Amplitude-Phase Recombination: Rethinking Robustness of Convolutional Neural Networks in Frequency Domain](https://openaccess.thecvf.com/content/ICCV2021/html/Chen_Amplitude-Phase_Recombination_Rethinking_Robustness_of_Convolutional_Neural_Networks_in_Frequency_ICCV_2021_paper.html)
In this paper, we notice that the CNN tends to converge at the local optimum which is closely related to the high-frequency components of the training images, while the amplitude spectrum is easily disturbed such as noises or common corruptions. In contrast, more empirical studies found that humans rely on more phase components to achieve robust recognition. This observation leads to more explanations of the CNN's generalization behaviors in both robustness to common perturbations and out-of-distribution detection, and motivates a new perspective on data augmentation designed by re-combing the phase spectrum of the current image and the amplitude spectrum of the distracter image. That is, the generated samples force the CNN to pay more attention to the structured information from phase components and keep robust to the variation of the amplitude.

- A way to augment data

### [Group-Wise Inhibition Based Feature Regularization for Robust Classification](https://openaccess.thecvf.com/content/ICCV2021/html/Liu_Group-Wise_Inhibition_Based_Feature_Regularization_for_Robust_Classification_ICCV_2021_paper.html)

- extract the feature and group features channel-wise, and calculate the importance of each feature
- dynamically suppress significant activation values of CNN by group-wise inhibition, but not fixedly or randomly handle them when training
- need training
- use the idea of masked feature for prediction

## Adversarial Attacks

### [Interpreting Attributions and Interactions of Adversarial Attacks](https://openaccess.thecvf.com/content/ICCV2021/html/Wang_Interpreting_Attributions_and_Interactions_of_Adversarial_Attacks_ICCV_2021_paper.html)

1. estimate attributions of different image regions to the decrease of the attacking cost based on the
Shapley value
2. define and quantify interactions among
adversarial perturbation pixels, and decompose the entire perturbation map into *relatively independent perturbation components*

### [Adversarial Attacks Are Reversible With Natural Supervision](https://openaccess.thecvf.com/content/ICCV2021/html/Mao_Adversarial_Attacks_Are_Reversible_With_Natural_Supervision_ICCV_2021_paper.html)

- defense in training time is not enough
- attack damges the incidential structure
- constractive learning
- inference-time defense by restoring the intrinsic structure of the input
- Reverse Attack: using attack to defense 

### [Feature Importance-aware Transferable Adversarial Attacks](https://openaccess.thecvf.com/content/ICCV2021/papers/Wang_Feature_Importance-Aware_Transferable_Adversarial_Attacks_ICCV_2021_paper.pdf)

- use the idea of the saliency map to weight the feature, and sum up the feature to construct the loss function
- minimize the feature that important to the prediction 

## Adversarial Defense

### [Dynamic Divide-and-Conquer Adversarial Training for Robust Semantic Segmentation](https://openaccess.thecvf.com/content/ICCV2021/html/Xu_Dynamic_Divide-and-Conquer_Adversarial_Training_for_Robust_Semantic_Segmentation_ICCV_2021_paper.html)

- sematic segmentation task
- divide pixels in safe trainning region and perturbation sensitive region which close to boundary
- main branch; mask branch and auxiliary branch
- mask branch give the mask to combine the safe rigion feature in main branch and sensitive region feature in auxiliary branch

### [Calibrated Adversarial Refinement for Stochastic Semantic Segmentation](https://openaccess.thecvf.com/content/ICCV2021/html/Kassapis_Calibrated_Adversarial_Refinement_for_Stochastic_Semantic_Segmentation_ICCV_2021_paper.html)

- propose a novel two-stage, cascaded approach for calibrated adversarial refinement: (i) a standard segmentation network is trained with categorical cross-entropy to predict a pixelwise probability distribution over semantic classes and (ii) an adversarially trained stochastic network is used to model the inter-pixel correlations to refine the output of the first network into coherent samples. 
- Importantly, to calibrate the refinement network and prevent mode collapse, the expectation of the samples in the second stage is matched to the probabilities predicted in the first.

### [Adversarial Example Detection Using Latent Neighborhood Graph](https://openaccess.thecvf.com/content/ICCV2021/html/Abusnaina_Adversarial_Example_Detection_Using_Latent_Neighborhood_Graph_ICCV_2021_paper.html)



## Interpretability

##### [Generic Attention-Model Explainability for Interpreting Bi-Modal and Encoder-Decoder Transformers](https://openaccess.thecvf.com/content/ICCV2021/html/Chefer_Generic_Attention-Model_Explainability_for_Interpreting_Bi-Modal_and_Encoder-Decoder_Transformers_ICCV_2021_paper.html)

- transformer-based model
- transformer interpretability
- task combine text and images
- aggregating attention heads (integrated gradients + attention maps)
- co-attention between images and texts

##### [Self-Supervised Geometric Features Discovery via Interpretable Attention for Vehicle Re-Identification and Beyond](https://openaccess.thecvf.com/content/ICCV2021/papers/Li_Self-Supervised_Geometric_Features_Discovery_via_Interpretable_Attention_for_Vehicle_Re-Identification_ICCV_2021_paper.pdf)

- for the task Vehicle-re-identification
- get geometric local features  global representation
- use self-supervised representation

### [Interpretable Image Recognition by Constructing Transparent Embedding Space](https://openaccess.thecvf.com/content/ICCV2021/html/Wang_Interpretable_Image_Recognition_by_Constructing_Transparent_Embedding_Space_ICCV_2021_paper.html)

- transparent embedding space
- grassmann manifold
- objective: construct the basis concpets and build the dicesion function
- embedding space learning -> construct the basic conceots: (1)orthonormilay loss, basis vectors should be indipendent to each other; (2) separation for class-aware subspace, maximize the projection metric amoing each pair of subspaces; (3) hight-level patches grouping, compact patches from same concept and separat patches from different concpet; (4) identification: classification cross entropy
- embedding space transparent -> know the meaning of each basic concept
- concept based classification

### [Stochastic Partial Swap: Enhanced Model Generalization and Interpretability for Fine-Grained Recognition](https://openaccess.thecvf.com/content/ICCV2021/html/Huang_Stochastic_Partial_Swap_Enhanced_Model_Generalization_and_Interpretability_for_Fine-Grained_ICCV_2021_paper.html)

- learn semantic representation (mid-level) with less cost
- injecting noise into features during training to suppress over response of some filters and encourage filters to jointly represent a concept
- randomly select sample from the same mini-batch as noise source and swap their partical feature elements element-wise
- compare to dropout, it claims "it equips a regularization effect similar to dropout"
- compute each region's contribution score to the prediction; generate the attention map corresponding each location i

### [*Statistically Consistent Saliency Estimation*](https://openaccess.thecvf.com/content/ICCV2021/papers/Luo_Statistically_Consistent_Saliency_Estimation_ICCV_2021_paper.pdf)

- statistically valid technique for model-agnostic saliency estimation: perturb images and find the most important pixel for the classifier
- linearly estimated gradient: f(x): the difference of two class probabilities; LEG: arg min_g E_{x~F+x_o}[(f(x)-f(x_o)-vec(g)^Tvec(x_o-x))^2]
- local approximation, using TV as constraint
- linear problem

### [Finding Representative Interpretations onConvolutional Neural Networks](https://openaccess.thecvf.com/content/ICCV2021/papers/Lam_Finding_Representative_Interpretations_on_Convolutional_Neural_Networks_ICCV_2021_paper.pdf)

- unsupervised approach to produce a highly representative interpretation
- The goal is to find and visualize the common decision logic of a CNN that governs the predictions on an input image
- approximiate the convex polytope inside decision boundary of netowrk
- 1) candidate pool generation; 2) submodular optimization; 3) semantic distance ranking 

### [Attentional Pyramid Pooling of Salient Visual Residuals for Place Recognition](https://openaccess.thecvf.com/content/ICCV2021/html/Peng_Attentional_Pyramid_Pooling_of_Salient_Visual_Residuals_for_Place_Recognition_ICCV_2021_paper.html)

- retreival task
- incorporates three types of attention modules to model the saliency of local feature in individual, spatial and cluster dimensions respectively
- (1) To inhibit task-irrelevant local features, a semantic-reinforced local weighting scheme is employed for local feature refinement;
- (2) To leverage the spatial context, an attentional pyramid structure is constructed to adaptively encode regional features according to their relative spatial saliency;
- (3) To distinguish the different importance of visual clusters to the task, a parametric normalization is proposed to adjust their contribution to image descriptor generation.

## Robustness

### [Amplitude-Phase Recombination: Rethinking Robustness of Convolutional Neural Networks in Frequency Domain](https://openaccess.thecvf.com/content/ICCV2021/html/Chen_Amplitude-Phase_Recombination_Rethinking_Robustness_of_Convolutional_Neural_Networks_in_Frequency_ICCV_2021_paper.html)
In this paper, we notice that the CNN tends to converge at the local optimum which is closely related to the high-frequency components of the training images, while the amplitude spectrum is easily disturbed such as noises or common corruptions. In contrast, more empirical studies found that humans rely on more phase components to achieve robust recognition. This observation leads to more explanations of the CNN's generalization behaviors in both robustness to common perturbations and out-of-distribution detection, and motivates a new perspective on data augmentation designed by re-combing the phase spectrum of the current image and the amplitude spectrum of the distracter image. That is, the generated samples force the CNN to pay more attention to the structured information from phase components and keep robust to the variation of the amplitude.

A way to augment data

## Adversarial Attacks

### [Interpreting Attributions and Interactions of Adversarial Attacks](https://openaccess.thecvf.com/content/ICCV2021/html/Wang_Interpreting_Attributions_and_Interactions_of_Adversarial_Attacks_ICCV_2021_paper.html)

1. estimate attributions of different image regions to the decrease of the attacking cost based on the
Shapley value
2. define and quantify interactions among
adversarial perturbation pixels, and decompose the entire perturbation map into *relatively independent perturbation components*

### [Adversarial Attacks Are Reversible With Natural Supervision](https://openaccess.thecvf.com/content/ICCV2021/html/Mao_Adversarial_Attacks_Are_Reversible_With_Natural_Supervision_ICCV_2021_paper.html)

- defense in training time is not enough
- attack damges the incidential structure
- constractive learning
- inference-time defense by restoring the intrinsic structure of the input
- Reverse Attack: using attack to defense 

## Interpretability

### [Generic Attention-Model Explainability for Interpreting Bi-Modal and Encoder-Decoder Transformers](https://openaccess.thecvf.com/content/ICCV2021/html/Chefer_Generic_Attention-Model_Explainability_for_Interpreting_Bi-Modal_and_Encoder-Decoder_Transformers_ICCV_2021_paper.html)

- transformer-based model
- transformer interpretability
- task combine text and images
- aggregating attention heads (integrated gradients + attention maps)
- co-attention between images and texts

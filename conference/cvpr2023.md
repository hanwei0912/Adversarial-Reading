## Adversarial

Towards Compositional Adversarial Robustness: Generalizing Adversarial Training to Composite Semantic Perturbations

Revisiting Residual Networks for Adversarial Robustness

CFA: Class-Wise Calibrated Fair Adversarial Training

Weakly Supervised Semantic Segmentation via Adversarial Learning of Classifier and Reconstructor

Jedi: Entropy-Based Localization and Removal of Adversarial Patches

Towards Effective Adversarial Textured 3D Meshes on Physical Face Recognition

Sibling-Attack: Rethinking Transferable Adversarial Attacks Against Face Recognition

Physically Adversarial Infrared Patches With Learnable Shapes and Locations

Physical-World Optical Adversarial Attacks on 3D Face Recognition

TWINS: A Fine-Tuning Framework for Improved Transferability of Adversarial Robustness and Generalization

SlowLiDAR: Increasing the Latency of LiDAR-Based Detection Using Adversarial Examples

Open-Set Semantic Segmentation for Point Clouds via Adversarial Prototype Framework

Adversarial Counterfactual Visual Explanations

Learning With Noisy Labels via Self-Supervised Adversarial Noisy Masking

RIATIG: Reliable and Imperceptible Adversarial Text-to-Image Generation With Natural Prompts

Proximal Splitting Adversarial Attack for Semantic Segmentation


The Enemy of My Enemy Is My Friend: Exploring Inverse Adversaries for Improving Adversarial Training

Towards Transferable Targeted Adversarial Examples

Boundary-Aware Backward-Compatible Representation via Adversarial Learning in Image Retrieval

Demystifying Causal Features on Adversarial Examples and Causal Inoculation for Robust Network by Adversarial Instrumental Variable Regression

Discrete Point-Wise Attack Is Not Enough: Generalized Manifold Adversarial Attack for Face Recognition

Transferable Adversarial Attacks on Vision Transformers With Token Gradient Regularization

Bi-Directional Feature Fusion Generative Adversarial Network for Ultra-High Resolution Pathological Image Virtual Re-Staining

Robust Single Image Reflection Removal Against Adversarial Attacks

Privacy-Preserving Adversarial Facial Features

Exploring the Relationship Between Architectural Design and Adversarially Robust Generalization

Boosting Accuracy and Robustness of Student Models via Adaptive Adversarial Distillation

StyLess: Boosting the Transferability of Adversarial Examples

Adversarially Robust Neural Architecture Search for Graph Neural Networks

Dual-Bridging With Adversarial Noise Generation for Domain Adaptive rPPG Estimation

Improving the Transferability of Adversarial Samples by Path-Augmented Method

PEFAT: Boosting Semi-Supervised Medical Image Classification via Pseudo-Loss Estimation and Feature Adversarial Training

Randomized Adversarial Training via Taylor Expansion

Semi-Supervised Hand Appearance Recovery via Structure Disentanglement and Dual Adversarial Discrimination

FIANCEE: Faster Inference of Adversarial Networks via Conditional Early Exits

Black-Box Sparse Adversarial Attack via Multi-Objective Optimisation

Progressive Backdoor Erasing via Connecting Backdoor and Adversarial Attacks

Edges to Shapes to Concepts: Adversarial Augmentation for Robust Vision

Feature Separation and Recalibration for Adversarial Robustness

Towards Benchmarking and Assessing Visual Naturalness of Physical World Adversarial Attacks

CLIP2Protect: Protecting Facial Privacy Using Text-Guided Makeup via Adversarial Latent Search

BiasAdv: Bias-Adversarial Augmentation for Model Debiasing

Learning To Retain While Acquiring: Combating Distribution-Shift in Adversarial Data-Free Knowledge Distillation

Adversarial Robustness via Random Projection Filters

Masked Auto-Encoders Meet Generative Adversarial Networks and Beyond

The Best Defense Is a Good Offense: Adversarial Augmentation Against Adversarial Attacks

Adversarially Masking Synthetic To Mimic Real: Adaptive Noise Injection for Point Cloud Segmentation Adaptation

Seasoning Model Soups for Robustness to Adversarial and Natural Distribution Shifts

Introducing Competition To Boost the Transferability of Targeted Adversarial Examples Through Clean Feature Mixup

StyleAdv: Meta Style Adversarial Training for Cross-Domain Few-Shot Learning

Multi-View Adversarial Discriminator: Mine the Non-Causal Factors for Object Detection in Unseen Domains

AGAIN: Adversarial Training With Attribution Span Enlargement and Hybrid Feature Fusion

Adversarial Normalization: I Can Visualize Everything (ICE)

Evading DeepFake Detectors via Adversarial Statistical Consistency

Evading Forensic Classifiers With Attribute-Conditioned Adversarial Faces

TeSLA: Test-Time Self-Learning With Automatic Adversarial Augmentation

## interpretability

Initialization Noise in Image Gradients and Saliency Maps

Don't Lie to Me! Robust and Efficient Explainability With Verified Perturbation Analysis

Are Data-Driven Explanations Robust Against Out-of-Distribution Data?

Improving Visual Grounding by Encouraging Consistent Gradient-Based Explanations

Language in a Bottle: Language Model Guided Concept Bottlenecks for Interpretable Image Classification

Reliable and Interpretable Personalized Federated Learning

OCTET: Object-Aware Counterfactual Explanations

Gradient-Based Uncertainty Attribution for Explainable Bayesian Deep Learning

Grounding Counterfactual Explanation of Image Classifiers to Textual Concept Space

S3C: Semi-Supervised VQA Natural Language Explanation via Self-Critical Learning

Toward Stable, Interpretable, and Lightweight Hyperspectral Super-Resolution

PIP-Net: Patch-Based Intuitive Prototypes for Interpretable Image Classification

CRAFT: Concept Recursive Activation FacTorization for Explainability

EVAL: Explainable Video Anomaly Localization

SketchXAI: A First Look at Explainability for Human Sketches

Spatial-Temporal Concept Based Explanation of 3D ConvNets

Interactive and Explainable Region-Guided Radiology Report Generation

A Loopback Network for Explainable Microvascular Invasion Classification

IDGI: A Framework To Eliminate Explanation Noise From Integrated Gradients

Explaining Image Classifiers With Multiscale Directional Image Representation

Bridging the Gap Between Model Explanations in Partially Annotated Multi-Label Classification

Affection: Learning Affective Explanations for Real-World Visual Data

Zero-Shot Everything Sketch-Based Image Retrieval, and in Explainable Style

Shortcomings of Top-Down Randomization-Based Sanity Checks for Evaluations of Deep Neural Network Explanations

Overlooked Factors in Concept-Based Explanations: Dataset Choice, Concept Learnability, and Human Capability

Global Vision Transformer Pruning With Hessian-Aware Saliency

TempSAL - Uncovering Temporal Information for Deep Saliency Prediction

Discriminative Co-Saliency and Background Mining Transformer for Co-Salient Object Detection

Sketch2Saliency: Learning To Detect Salient Objects From Human Drawings

Learning From Unique Perspectives: User-Aware Saliency Modeling

ASP-Net: Rethinking Video Saliency Prediction From an Audio-Visual Consistency Perceptual Perspective

Texture-Guided Saliency Distilling for Unsupervised Salient Object Detection

Realistic Saliency Guided Image Enhancement

Boosting Low-Data Instance Segmentation by Unsupervised Pre-Training With Saliency Prompt

## Ronan's list

### [Explaining Bayesian Neural Networks](https://arxiv.org/pdf/2108.10346.pdf)
In this work, we bring
together these two perspectives of transparency into a holistic
explanation framework for explaining BNNs. Within the Bayesian
framework, the network weights follow a probability distribution.
Hence, the standard (deterministic) prediction strategy of DNNs
extends in BNNs to a predictive distribution, and thus the
standard explanation extends to an explanation distribution.
Exploiting this view, we uncover that BNNs implicitly employ
multiple heterogeneous prediction strategies. While some of these
are inherited from standard DNNs, others are revealed to us by
considering the inherent uncertainty in BNNs.

contributions:
- provide theoretical justification along with a detailed practical explanation for usage of the mean explanation as most simplistic option to explain the decision making process of a BNN
- proposed UAI (Union and Intersection Explanation)
- investigate multi-modality
- Generality

comments:
The idea of UAI is interesting, it might useful for our work.

### [DORA: Exploring outlier representations in Deep Neural Networks](https://arxiv.org/pdf/2206.04530.pdf)
In this work, we introduce DORA (Data-agnOstic
Representation Analysis): the first data-agnostic framework for the analysis of the
representation space of DNNs. Our framework employs the proposed Extreme-Activation
(EA) distance measure between representations that utilizes self-explaining capabilities
within the network without accessing any data. We quantitatively validate the metricâ€™s
correctness and alignment with human-defined semantic distances. The coherence
between the EA distance and human judgment enables us to identify representations
whose underlying concepts would be considered unnatural by humans by identifying
outliers in functional distance. Finally, we demonstrate the practical usefulness of
DORA by analyzing and identifying artifact representations in popular Computer Vision
models.

contributions:
- extreme-activation distance metric for representations: they assume the neural representation satisfies normal distrubution; 1) activation maximisation signals 2) representation activation vectors: mean activation of fb given the n-AMS of fa; extreme-activation distance is 1/sqrt(2)*sqrt(1-cos(rij,rji)), where rij,rji is thier pair-wise RAV
- DORA framework:
- quantitatively assess

comments:
It use the class of data as part of explanation, but really distance of representation is explanable?????

### [Quantus: An Explainable AI Toolkit for Responsible Evaluation of Neural Network Explanations and Beyond](https://www.jmlr.org/papers/volume24/22-0142/22-0142.pdf)

comments:
:) 

### [Saliency Cards: A Framework to Characterize and Compare Saliency Methods](https://dl.acm.org/doi/pdf/10.1145/3593013.3593997)
In response, we introduce saliency
cards: structured documentation of how saliency methods operate and their performance across a battery of evaluative metrics.
Through a review of 25 saliency method papers and 33 method
evaluations, we identify 10 attributes that users should account for
when choosing a method. We group these attributes into three categories that span the process of computing and interpreting saliency:
methodology, or how the saliency is calculated; sensitivity, or the
relationship between the saliency and the underlying model and
data; and, perceptibility, or how an end user ultimately interprets
the result. By collating this information, saliency cards allow users
to more holistically assess and compare the implications of different
methods. Through nine semi-structured interviews with users from
various backgrounds, including researchers, radiologists, and computational biologists, we find that saliency cards provide a detailed
vocabulary for discussing individual methods and allow for a more
systematic selection of task-appropriate methods

![](figures/saliencycard.png)

[code](https://github.com/mitvis/saliency-cards)

### [Beyond Expertise and Roles: A Framework to Characterize the Stakeholders of Interpretable Machine Learning and their Needs](https://dl.acm.org/doi/10.1145/3411764.3445088)

### [Benchmarking and survey of explanation methods for black box models](https://link.springer.com/article/10.1007/s10618-023-00933-9)
Unsurprisingly, the state-of-the-art exhibits currently a plethora of explainers providing many different types of explanations. With the aim of providing a compass for researchers and practitioners, this paper proposes a categorization of explanation methods from the perspective of the type of explanation they return, also considering the different input data formats. The paper accounts for the most representative explainers to date, also discussing similarities and discrepancies of returned explanations through their visual appearance. A companion website to the paper is provided as a continuous update to new explainers as they appear. Moreover, a subset of the most robust and widely adopted explainers, are benchmarked with respect to a repertoire of quantitative metrics.

### [Seg-XRes-CAM: Explaining Spatially Local Regions in Image Segmentation](https://openaccess.thecvf.com/content/CVPR2023W/XAI4CV/papers/Hasany_Seg-XRes-CAM_Explaining_Spatially_Local_Regions_in_Image_Segmentation_CVPRW_2023_paper.pdf)

- based on HiResCAM (element-wise grad-CAM), add pooling and upsampling

comments:
.............

### [Use HiResCAM instead of Grad-CAM for faithful explanations of convolutional neural networks](https://arxiv.org/pdf/2011.08891.pdf)

### [Shared Interest...Sometimes: Understanding the Alignment between Human Perception, Vision Architectures, and Saliency Map Techniques](https://xai4cv.github.io/assets/papers2023/P16_SharedInterestSometimes.pdf)

### [PIP-Net: Patch-Based Intuitive Prototypes for Interpretable Image Classification](https://openaccess.thecvf.com/content/CVPR2023/papers/Nauta_PIP-Net_Patch-Based_Intuitive_Prototypes_for_Interpretable_Image_Classification_CVPR_2023_paper.pdf)

### [A Living and Curated Collection of Explainable AI Methods](https://utwente-dmb.github.io/xai-papers/#/)


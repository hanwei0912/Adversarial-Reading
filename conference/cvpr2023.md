## Ronan's list

### [Explaining Bayesian Neural Networks](https://arxiv.org/pdf/2108.10346.pdf)
In this work, we bring
together these two perspectives of transparency into a holistic
explanation framework for explaining BNNs. Within the Bayesian
framework, the network weights follow a probability distribution.
Hence, the standard (deterministic) prediction strategy of DNNs
extends in BNNs to a predictive distribution, and thus the
standard explanation extends to an explanation distribution.
Exploiting this view, we uncover that BNNs implicitly employ
multiple heterogeneous prediction strategies. While some of these
are inherited from standard DNNs, others are revealed to us by
considering the inherent uncertainty in BNNs.

contributions:
- provide theoretical justification along with a detailed practical explanation for usage of the mean explanation as most simplistic option to explain the decision making process of a BNN
- proposed UAI (Union and Intersection Explanation)
- investigate multi-modality
- Generality

comments:
The idea of UAI is interesting, it might useful for our work.

### [DORA: Exploring outlier representations in Deep Neural Networks](https://arxiv.org/pdf/2206.04530.pdf)
In this work, we introduce DORA (Data-agnOstic
Representation Analysis): the first data-agnostic framework for the analysis of the
representation space of DNNs. Our framework employs the proposed Extreme-Activation
(EA) distance measure between representations that utilizes self-explaining capabilities
within the network without accessing any data. We quantitatively validate the metricâ€™s
correctness and alignment with human-defined semantic distances. The coherence
between the EA distance and human judgment enables us to identify representations
whose underlying concepts would be considered unnatural by humans by identifying
outliers in functional distance. Finally, we demonstrate the practical usefulness of
DORA by analyzing and identifying artifact representations in popular Computer Vision
models.

contributions:
- extreme-activation distance metric for representations: they assume the neural representation satisfies normal distrubution; 1) activation maximisation signals 2) representation activation vectors: mean activation of fb given the n-AMS of fa; extreme-activation distance is 1/sqrt(2)*sqrt(1-cos(rij,rji)), where rij,rji is thier pair-wise RAV
- DORA framework:
- quantitatively assess

comments:
It use the class of data as part of explanation, but really distance of representation is explanable?????

### [Quantus: An Explainable AI Toolkit for Responsible Evaluation of Neural Network Explanations and Beyond](https://www.jmlr.org/papers/volume24/22-0142/22-0142.pdf)

comments:
:) 

### [Saliency Cards: A Framework to Characterize and Compare Saliency Methods](https://dl.acm.org/doi/pdf/10.1145/3593013.3593997)
In response, we introduce saliency
cards: structured documentation of how saliency methods operate and their performance across a battery of evaluative metrics.
Through a review of 25 saliency method papers and 33 method
evaluations, we identify 10 attributes that users should account for
when choosing a method. We group these attributes into three categories that span the process of computing and interpreting saliency:
methodology, or how the saliency is calculated; sensitivity, or the
relationship between the saliency and the underlying model and
data; and, perceptibility, or how an end user ultimately interprets
the result. By collating this information, saliency cards allow users
to more holistically assess and compare the implications of different
methods. Through nine semi-structured interviews with users from
various backgrounds, including researchers, radiologists, and computational biologists, we find that saliency cards provide a detailed
vocabulary for discussing individual methods and allow for a more
systematic selection of task-appropriate methods

![](figures/saliencycard.png)

[code](https://github.com/mitvis/saliency-cards)

### [Beyond Expertise and Roles: A Framework to Characterize the Stakeholders of Interpretable Machine Learning and their Needs](https://dl.acm.org/doi/10.1145/3411764.3445088)

### [Benchmarking and survey of explanation methods for black box models](https://link.springer.com/article/10.1007/s10618-023-00933-9)
Unsurprisingly, the state-of-the-art exhibits currently a plethora of explainers providing many different types of explanations. With the aim of providing a compass for researchers and practitioners, this paper proposes a categorization of explanation methods from the perspective of the type of explanation they return, also considering the different input data formats. The paper accounts for the most representative explainers to date, also discussing similarities and discrepancies of returned explanations through their visual appearance. A companion website to the paper is provided as a continuous update to new explainers as they appear. Moreover, a subset of the most robust and widely adopted explainers, are benchmarked with respect to a repertoire of quantitative metrics.

### [Seg-XRes-CAM: Explaining Spatially Local Regions in Image Segmentation](https://openaccess.thecvf.com/content/CVPR2023W/XAI4CV/papers/Hasany_Seg-XRes-CAM_Explaining_Spatially_Local_Regions_in_Image_Segmentation_CVPRW_2023_paper.pdf)

- based on HiResCAM (element-wise grad-CAM), add pooling and upsampling

comments:
.............

### [Use HiResCAM instead of Grad-CAM for faithful explanations of convolutional neural networks](https://arxiv.org/pdf/2011.08891.pdf)

### [Shared Interest...Sometimes: Understanding the Alignment between Human Perception, Vision Architectures, and Saliency Map Techniques](https://xai4cv.github.io/assets/papers2023/P16_SharedInterestSometimes.pdf)


## ICML 2020

#### [Perceptual Generative Autoencoders](https://proceedings.icml.cc/static/paper_files/icml/2020/1042-Paper.pdf)

- Target: to increase the discrepancy of the generative model. Solution: to map both the generated and target distributions to a latent space using the encoder of a standard autoencoder, and train the generator to match the target distribution in the latent space. They enforece the consistency in both the data space and the latent space with theoretically justifed data and latent reconstruction losses.

#### [NGBoost: Natural Gradient Boosting for Probabilistic Prediction](https://proceedings.icml.cc/static/paper_files/icml/2020/3337-Paper.pdf)


#### [Continuous Graph Neural Networks](https://proceedings.icml.cc/static/paper_files/icml/2020/4075-Paper.pdf)

#### [Understanding Contrastive Representation Learning through Alignment and Uniformity on the Hypersphere](https://proceedings.icml.cc/static/paper_files/icml/2020/5503-Paper.pdf)

- Constrastive representation learning: 

Contrastive methods learn the feature by comparing the positive samples and the negtive samples in the feature space. 
They identify two key properties related to the contrastive loss: (1) alignment (closeness) of featrues from positive pairs, (2) uniformity of the induced distribution of the normalized features on the hypersphere.


## 10 interesting papers

### [Autoregressive Diffusion Models](https://openreview.net/forum?id=Lm8T39vLDTE)
In handwavy terms, Diffusion Models generate images by iteratively adding “differentiable noise” on a pixel grid that eventually becomes a real-looking image. Inference starts by sampling some kind of “white noise” image. This work proposes to do a similar process but instead of applying the diffusion step to iteratively decode all pixels at the same time, they decode a few pixels at a time autoregressively which then remain fixed for the rest of the process

### [Poisoning and Backdooring Contrastive Learning](https://openreview.net/forum?id=iC4UHbQ01Mp)
This paper explores how an adversary might poison a small subset of the training data for a model like CLIP — trained using contrastive learning on image-text pairs from the web — such that the model will misclassify test images. They try 2 methods to do so:

Targeted poisoning: the training dataset is modified by adding poisoned samples with the goal of having the end model misclassify a particular image with a wrong specific label. According to the results, this can be consistently achieved by just poisoning a 0.0001% of the training dataset, e.g. adding 3 image pairs to a dataset of 3 million instances.
Backdoor attack: instead of having a particular target image, this approach aims to overlay a small patch of pixels on any image such that this will be misclassified with a desired wrong label. This more ambitious attack can be pulled off consistently by poisoning 0.01% of the training dataset, e.g. poisoning 300 images out of a 3 million instances dataset.

### [Bootstrapped Meta-Learning](https://openreview.net/forum?id=b-ny3x071E5)
In meta-learning, the learner is equipped with an outer loop of optimization that optimizes the “learning rule” of the inner optimization, which directly optimizes a learning objective (e.g. via gradient descent). In terribly oversimplified terms, existing meta-learning algorithms often rely on the performance of the learner to evaluate a learning rule: run the learner for k steps, if the learning improves do more of that, if the learning gets worse, do less of that. The problem with directly using the learner’s objective is that the meta-learning optimization will (1) be constrained to the same geometry of the learning objective function and (2) the optimization will be myopic, given that it will only optimize for a horizon of k steps, whereas the dynamics of learning beyond that might be much more complex.

Frankly, the theoretical details of this process go over my head, but the gist of it is that the meta-learner is first asked to predict the performance of the learner beyond the evaluated k-steps, and then it optimizes following that very prediction; in other words, the meta-learner generates its own target to optimize. This enables the meta-learner to optimize for a longer time horizon without the need to actually evaluate such long time horizons which is computationally expensive.

### [Equivariant Subgraph Aggregation Networks](https://openreview.net/forum?id=dFbKQaRk15w)
How do you know if two graphs are the same? You might think just looking at them is enough, but you’d be wrong. The same graph can be represented in different ways by reorganizing or permitting the order of nodes such that given two graphs it can be hard to identify whether they are the same, namely isomorphic.

The Weisfeiler-Leman (WL) test is an algorithm that recursively classifies the nodes of a graph based on its immediate neighborhood. If after all these processes the nodes of the two graphs have “different classifications” this means the test failed, implying the two graphs are different (non-isomorphic). On the other hand, if the two graphs are “still the same” after the WL test, they are probably isomorphic, but it’s not guaranteed! There are certain graph structures the WL test will fail to differentiate.

### [Perceiver IO: A General Architecture for Structured Inputs & Outputs](https://openreview.net/forum?id=fILj7WpI-g)
This work follows a similar line as the original Perceiver³ by augmenting it with a flexible querying mechanism which lets the model have an output of arbitrary size instead of requiring a task-specific architecture at the end of the model. that enables outputs of various sizes and semantics, doing away with the need for task-specific architecture engineering.

### [Exploring the Limits of Large Scale Pre-training](https://openreview.net/forum?id=V3C8p78sDa)
They study how pre-training performance on Upstream (US) tasks (e.g. large-scale ImageNet labels) transfers to Downstream (DS) performance (e.g. whale detection). Then do this experiment for a lot — by a lot mean a lot — of architectures and sizes.

### [Language modeling via stochastic processes](https://openreview.net/forum?id=pMQwKL1yctf)

### [Coordination Among Neural Modules Through a Shared Global Workspace](https://openreview.net/forum?id=XzTtHjgPDsT)
The Global Workspace Theory (GWT) is a proposed cognitive architecture to account for how conscious and unconscious thought processes manifest in humans. One of its core assumptions is the existence of a shared workspace that all specialist modules have access to, enabling coherence between otherwise isolated modules. This paper conceptualizes a neural network architecture where a set of inputs are processed by expert neural networks, which are then written into a shared workspace — a set of vectors — which are then broadcasted to the experts again.

### [Learning Fast, Learning Slow: A General Continual Learning Method based on Complementary Learning System](https://openreview.net/forum?id=uxxFrDwrE7Y)
Continual Learning is a method for having a model gradually extend its knowledge by exposing it to new data or interacting with a dynamic environment. As an example, think about a model that initially only learns to classify images with digits from 0 to 7, and is taught to recognize digits 8 and 9, without forgetting about the previous digits. The goal is to be able to leverage existing knowledge to learn more efficiently about new things, just like humans do.

### [Autonomous Reinforcement Learning: Formalism and Benchmarking](https://openreview.net/forum?id=nkaba3ND7B5)
This work proposes a benchmark that focuses on non-episodic RL which the authors call Environments for Autonomous Reinforcement Learning (EARL) with the hope that it resembles the real world.

## Interpretability

### [DISCOVERING AND EXPLAINING THE REPRESENTATION BOTTLENECK OF DNNS](https://openreview.net/pdf?id=iRCUlgmdfHJ)

### [Understanding Latent Correlation-Based Multiview Learning and Self-Supervision: An Identifiability Perspective](https://openreview.net/forum?id=5FUq05QRc5b)

### [Interpretable Unsupervised Diversity Denoising and Artefact Removal](https://openreview.net/forum?id=DfMqlB0PXjM)

### [Explanations of Black-Box Models based on Directional Feature Interactions](https://openreview.net/forum?id=45Mr7LeKR9)

### [Understanding and Preventing Capacity Loss in Reinforcement Learning]

### [Understanding the Role of Self Attention for Efficient Speech Recognition]

### [Understanding and Leveraging Overparameterization in Recursive Value Estimation](https://openreview.net/forum?id=shbAgEsk3qM)

### [DEGREE: Decomposition Based Explanation for Graph Neural Networks](https://openreview.net/forum?id=Ve0Wth3ptT_)

### [DISSECT: Disentangled Simultaneous Explanations via Concept Traversals ]

### [Joint Shapley values: a measure of joint feature importance ](https://openreview.net/forum?id=vcUmUvQCloe)

### [Bridging the Gap: Providing Post-Hoc Symbolic Explanations for Sequential Decision-Making Problems with Inscrutable Representations](https://openreview.net/forum?id=o-1v9hdSult)

## Adversarial Attack

### [On Improving Adversarial Transferability of Vision Transformers](https://openreview.net/forum?id=D6nH3719vZy)

### [Adversarial Support Alignment](https://openreview.net/forum?id=26gKg6x-ie)

### [NODE-GAM: Neural Generalized Additive Model for Interpretable Deep Learning](https://openreview.net/forum?id=g8NJR6fCCl8)

### [Patch-Fool: Are Vision Transformers Always Robust Against Adversarial Perturbations?](https://openreview.net/forum?id=28ib9tf6zhr)

### [Evading Adversarial Example Detection Defenses with Orthogonal Projected Gradient Descent](https://openreview.net/forum?id=af1eUDdUVz)

## Robustness

### [How to Robustify Black-Box ML Models? A Zeroth-Order Optimization Perspective](https://openreview.net/forum?id=W9G_ImpHlQd)

### [Towards Understanding the Data Dependency of Mixup-style Training](https://openreview.net/forum?id=ieNJYujcGDO)

### [Improved deterministic l2 robustness on CIFAR-10 and CIFAR-100](https://openreview.net/forum?id=tD7eCtaSkR)

### [Self-supervised Learning is More Robust to Dataset Imbalance]

### [Finding Biological Plausibility for Adversarially Robust Features via Metameric Tasks]

### [Implicit Bias of Projected Subgradient Method Gives Provable Robust Recovery of Subspaces of Unknown Codimension](https://openreview.net/forum?id=vA7doMdgi75)

### [Byzantine-Robust Learning on Heterogeneous Datasets via Bucketing]

### [AEVA: Black-box Backdoor Detection Using Adversarial Extreme Value Analysis]

### [Reducing Excessive Margin to Achieve a Better Accuracy vs. Robustness Trade-off](https://openreview.net/forum?id=Azh9QBQ4tR7)

### [Towards Understanding the Robustness Against Evasion Attack on Categorical Data](https://openreview.net/forum?id=BmJV7kyAmg)

### [Provably Robust Adversarial Examples](https://openreview.net/forum?id=UMfhoMtIaP5)

### [Robust Learning Meets Generative Models: Can Proxy Distributions Improve Adversarial Robustness?]

## Privacy

### [RelaxLoss: Defending Membership Inference Attacks without Losing Utility](https://openreview.net/forum?id=FEDfGWVZYIn)

### [Improving Federated Learning Face Recognition via Privacy-Agnostic Clusters]

### [PEARL: Data Synthesis via Private Embeddings and Adversarial Reconstruction Learning]

### [COPA: Certifying Robust Policies for Offline Reinforcement Learning against Poisoning Attacks]

### [Robbing the Fed: Directly Obtaining Private Data in Federated Learning with Modified Models]

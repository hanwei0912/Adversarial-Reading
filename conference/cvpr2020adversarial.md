

## Attack

Towards Large Yet Imperceptible Adversarial Image Perturbations With Perceptual Color Distance

ColorFool: Semantic Adversarial Colorization

Polishing Decision-Based Adversarial Noise With a Customized Sampling

Transferable, Controllable, and Inconspicuous Adversarial Attacks on Person Re-identification With Deep Mis-Ranking

Boosting the Transferability of Adversarial Samples via Attention

Adversarial Texture Optimization From RGB-D Scans

Modeling Biological Immunity to Adversarial Examples

Enhancing Cross-Task Black-Box Transferability of Adversarial Examples With Dispersion Reduction

Adversarial Camouflage: Hiding Physical-World Attacks With Natural Styles

GeoDA: A Geometric Framework for Black-Box Adversarial Attacks

What Machines See Is Not What They Get: Fooling Scene Text Recognition Models With Adversarial Text Images

Physically Realizable Adversarial Examples for LiDAR Object Detection

One-Shot Adversarial Attacks on Visual Tracking With Dual Attention

Robust Superpixel-Guided Attentional Adversarial Attack

ILFO: Adversarial Attack on Adaptive Neural Networks

PhysGAN: Generating Physical-World-Resilient Adversarial Examples for Autonomous Driving

Smoothing Adversarial Domain Attack and P-Memory Reconsolidation for Cross-Domain Person Re-Identification

## Defense
### [One Man's Trash Is Another Man's Treasure: Resisting Adversarial Examples by Adversarial Examples](https://openaccess.thecvf.com/content_CVPR_2020/papers/Xiao_One_Mans_Trash_Is_Another_Mans_Treasure_Resisting_Adversarial_Examples_CVPR_2020_paper.pdf)
To defend
against adversarial examples, a plausible idea is to obfuscate the network’s gradient with respect to the input image.
This general idea has inspired a long line of defense methods.
Yet, almost all of them have proven vulnerable.
We revisit this seemingly flawed idea from a radically
different perspective. We embrace the omnipresence of adversarial examples and the numerical procedure of crafting
them, and turn this harmful attacking process into a useful
defense mechanism. Our defense method is conceptually
simple: before feeding an input image for classification,
transform it by finding an adversarial example on a pretrained external model.

### [Noise is Inside Me! Generating Adversarial Perturbations with Noise Derived from Natural Filters](https://openaccess.thecvf.com/content_CVPRW_2020/papers/w47/Agarwal_Noise_Is_Inside_Me_Generating_Adversarial_Perturbations_With_Noise_Derived_CVPRW_2020_paper.pdf)
- Camera Inspired Perturbations: the images always have noises which come from the environmental factors or camera noise incorporated, they embeded adversarial perturbation into this kind noise. They claim that their method can be applied at real-time. It is model-agnostic and can be utilized to fool multiple deep learning classifier on various databases.
- They use differet filter like median filter, guassion filter, wavelet filter. median filter is good at remove while peper noise, and when they use media filter inside, the adversarial perturbation looks like while peper noise. It seems they use the derivative version of the filter.

Single-Step Adversarial Training With Dropout Scheduling

Adversarial Examples Improve Image Recognition

Adversarial Robustness: From Self-Supervised Pre-Training to Fine-Tuning

Efficient Adversarial Training With Transferable Adversarial Examples

Towards Achieving Adversarial Robustness by Enforcing Feature Consistency Across Bit Planes

Benchmarking Adversarial Robustness on Image Classification

DaST: Data-Free Substitute Training for Adversarial Attacks

Ensemble Generative Cleaning With Feedback Loops for Defending Adversarial Attacks

When NAS Meets Robustness: In Search of Robust Architectures Against Adversarial Attacks

Enhancing Intrinsic Adversarial Robustness via Feature Pyramid Decoder

### [Defending and Harnessing the Bit-Flip Based Adversarial Weight Attack](https://openaccess.thecvf.com/content_CVPR_2020/papers/He_Defending_and_Harnessing_the_Bit-Flip_Based_Adversarial_Weight_Attack_CVPR_2020_paper.pdf)
In this work, we conduct comprehensive
investigations on BFA and propose to leverage binarizationaware training and its relaxation – piece-wise clustering as
simple and effective countermeasures to BFA.

Detecting Adversarial Samples Using Influence Functions and Nearest Neighbors

## Robust
Achieving Robustness in the Wild via Adversarial Mixing With Disentangled Representations

Adversarial Vertex Mixup: Toward Better Adversarially Robust Generalization

Learn2Perturb: An End-to-End Feature Perturbation Learning to Improve Adversarial Robustness

On Isometry Robustness of Deep 3D Point Cloud Models Under Adversarial Attacks

A Self-supervised Approach for Adversarial Robustness

Exploiting Joint Robustness to Adversarial Perturbations

Robust Design of Deep Neural Networks Against Adversarial Attacks Based on Lyapunov Theory

Understanding Adversarial Examples From the Mutual Influence of Images and Perturbations

## Other
DOA-GAN: Dual-Order Attentive Generative Adversarial Network for Image Copy-Move Forgery Detection and Localization

Multi-scale Domain-adversarial Multiple-instance CNN for Cancer Subtype Classification with Unannotated Histopathological Images

Progressive Adversarial Networks for Fine-Grained Domain Adaptation

Local Class-Specific and Global Image-Level Generative Adversarial Networks for Semantic-Guided Scene Generation

AdversarialNAS: Adversarial Neural Architecture Search for GANs

State-Relabeling Adversarial Active Learning

MSG-GAN: Multi-Scale Gradients for Generative Adversarial Networks

A U-Net Based Discriminator for Generative Adversarial Networks

EventSR: From Asynchronous Events to Image Reconstruction, Restoration, and Super-Resolution via End-to-End Adversarial Learning

Fashion Editing With Adversarial Parsing Learning

Non-Adversarial Video Synthesis With Learned Priors

CIAGAN: Conditional Identity Anonymization Generative Adversarial Networks

Noise Robust Generative Adversarial Networks

Bayesian Adversarial Human Motion Synthesis

ARShadowGAN: Shadow Generative Adversarial Network for Augmented Reality in Single Light Scenes

Distribution-Induced Bidirectional Generative Adversarial Network for Graph Representation Learning

End-to-End Adversarial-Attention Network for Multi-Modal Clustering

Adversarial Latent Autoencoders

Adversarial Feature Hallucination Networks for Few-Shot Learning

Deep Adversarial Decomposition: A Unified Framework for Separating Superimposed Images

## Adversarial Attack and Robustness

#### [Improving Arabic Diacritization with Regularized Decoding and Adversarial Training](https://aclanthology.org/2021.acl-short.68.pdf)

- task: arabic diacritization
- propose to use regularized decoding and adversarial training to appropriately learn from such noisy knowledge
for diacritization

#### [Are VQA Systems RAD? Measuring Robustness to Augmented Data with Focused Interventions](https://aclanthology.org/2021.acl-short.10.pdf)

- visual question answering, robustness to augmented data (consistency of model predictions between original and augmented examples)
- Our proposed augmentations are designed to make a
focused intervention on a specific property of
the question such that the answer changes. Using these augmentations, we propose a new robustness measure, Robustness to Augmented
Data (RAD), which measures the consistency
of model predictions between original and augmented examples

#### [An Empirical Study on Adversarial Attack on NMT: Languages and Positions Matter](https://aclanthology.org/2021.acl-short.58.pdf)

- task: neural machine translation
- For autoregressive NMT models that generate target words
from left to right, we observe that adversarial
attack on the source language is more effective
than on the target language, and that attacking front positions of target sentences or positions of source sentences aligned to the front
positions of corresponding target sentences is
more effective than attacking other positions.

#### [Using Adversarial Attacks to Reveal the Statistical Bias in Machine Reading Comprehension Models](https://aclanthology.org/2021.acl-short.43.pdf)

- task: machine reading comprehension
- apply the method to
the RACE dataset, for which the answer to each
MRC question is selected from 4 options. It is
found that several pre-trained language models, including BERT, ALBERT, and RoBERTa,
show consistent preference to some options,
even when these options are irrelevant to the
question.

#### [Robust Transfer Learning with Pretrained Language Models through Adapters](https://aclanthology.org/2021.acl-short.108.pdf)

- 
